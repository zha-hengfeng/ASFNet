import time
import torch

from torch.autograd import Variable
from utils.lr_scheduler import WarmupPolyLR


def train(args, train_loader, model, criterion, optimizer, epoch):
    """
    args:
       train_loader: loaded for training dataset
       model: model
       criterion: loss function
       optimizer: optimization algorithm, such as ADAM or SGD
       epoch: epoch number
    return: average loss, per class IoU, and mean IoU
    """
    model.train()
    epoch_loss = []

    total_batches = len(train_loader)
    print("=====> the number of iterations per epoch: ", total_batches)
    lambda1 = lambda epoch: pow((1 - ((epoch - 1) / args.max_epochs)), 0.9)  ## scheduler 2
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)
    scheduler.step(epoch)
    args.per_iter = total_batches
    args.max_iter = args.max_epochs * args.per_iter
    st = time.time()
    for iteration, batch in enumerate(train_loader, 0):
        args.cur_iter = epoch * args.per_iter + iteration
        # scheduler = WarmupPolyLR(optimizer, T_max=args.max_iter, cur_iter=args.cur_iter,
        #                          warmup_factor=1.0 / 3, warmup_iters=500, power=0.9)
        scheduler.step(epoch)
        lr = optimizer.param_groups[0]['lr']
        start_time = time.time()
        images, labels, _, _ = batch
        images = Variable(images).cuda()
        labels = Variable(labels.long()).cuda()
        output = model(images)
        loss = criterion(output, labels)

        optimizer.zero_grad()  # set the grad to zero
        loss.backward()
        optimizer.step()

        epoch_loss.append(loss.item())
        time_taken = time.time() - start_time

        print('=====> epoch[%d/%d] iter: (%d/%d) \tcur_lr: %.6f loss: %.3f time:%.2f' % (epoch + 1, args.max_epochs,
                                                                                         iteration + 1, total_batches,
                                                                                         lr, loss.item(), time_taken))

    time_taken_epoch = time.time() - st
    remain_time = time_taken_epoch * (args.max_epochs - 1 - epoch)
    m, s = divmod(remain_time, 60)
    h, m = divmod(m, 60)
    print("Remaining training time = %d hour %d minutes %d seconds" % (h, m, s))

    average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)

    return average_epoch_loss_train, lr
